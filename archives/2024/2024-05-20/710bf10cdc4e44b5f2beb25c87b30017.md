# [Published on 2024-05-20](index.md)

* [2024-05-20, 15:33:44](https://news.ycombinator.com/item?id=40416657) - [26Ã— Faster Inference with Layer-Condensed KV Cache for Large Language Models](https://arxiv.org/abs/2405.10637)
